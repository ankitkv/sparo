{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a44d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8eb437-f1e9-4781-b463-645bb57b994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"/path/to/checkpoint.pt\"\n",
    "SUGARCREPE_PATH  = \"/path/to/sugar-crepe\"\n",
    "COCO_PATH = \"/path/to/coco/val2017/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_paths = [CHECKPOINT_PATH]\n",
    "model_config = {\"L\": 64, \"V\": 64, \"reduce_depth\": 1, \"sparo_type\": \"cont:const\", \"share_kv\": True}\n",
    "model_type = \"ViT-B-16-SPARO\"\n",
    "num_edge_patches = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a9a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "random_seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "models = []\n",
    "for _ in pretrained_paths:\n",
    "    model, preprocess_train, preprocess_val = create_model_and_transforms(\n",
    "        model_type,\n",
    "        \"\",\n",
    "        precision=\"amp_bfloat16\",\n",
    "        device=\"cuda\",\n",
    "        jit=False,\n",
    "        force_quick_gelu=False,\n",
    "        force_custom_text=False,\n",
    "        force_patch_dropout=None,\n",
    "        force_image_size=None,\n",
    "        pretrained_image=False,\n",
    "        image_mean=None,\n",
    "        image_std=None,\n",
    "        aug_cfg={},\n",
    "        output_dict=True,\n",
    "        override_config=model_config,\n",
    "    )\n",
    "    model.eval()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pretrained_path in enumerate(pretrained_paths):\n",
    "    checkpoint = pt_load(pretrained_path, map_location='cpu')\n",
    "    sd = checkpoint[\"state_dict\"]\n",
    "    if next(iter(sd.items()))[0].startswith('module'):\n",
    "        sd = {k[len('module.'):]: v for k, v in sd.items()}\n",
    "    models[i].load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f52b9-2e67-4191-9f83-f71f37a78c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "data_dict = {\n",
    "    'add_obj'    : f'{SUGARCREPE_PATH}/data/add_obj.json',\n",
    "    'add_att'    : f'{SUGARCREPE_PATH}/data/add_att.json',\n",
    "    'replace_obj': f'{SUGARCREPE_PATH}/data/replace_obj.json',\n",
    "    'replace_att': f'{SUGARCREPE_PATH}/data/replace_att.json',\n",
    "    'replace_rel': f'{SUGARCREPE_PATH}/data/replace_rel.json',\n",
    "    'swap_obj'   : f'{SUGARCREPE_PATH}/data/swap_obj.json',\n",
    "    'swap_att'   : f'{SUGARCREPE_PATH}/data/swap_att.json',\n",
    "}\n",
    "dataset = {}\n",
    "for c, data_path in data_dict.items():\n",
    "    dataset[c] = json.load(open(data_path, 'r', encoding='utf-8'))\n",
    "\n",
    "\n",
    "class TextRetrievalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_root, data_dict):\n",
    "        self.image_root = image_root\n",
    "\n",
    "        self.datasets = list([list(v.values()) for v in data_dict.values()])\n",
    "        self.lengths = [len(v) for v in self.datasets]\n",
    "\n",
    "        self.dataset = sum(self.datasets, [])\n",
    "        self.length = len(self.dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        image_path = os.path.join(self.image_root, data['filename'])\n",
    "        image = Image.open(image_path)\n",
    "        return image, data['caption'], data['negative_caption']\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    images, captions, neg_captions = zip(*data)\n",
    "    return images, captions, neg_captions\n",
    "\n",
    "\n",
    "tokenizer = get_tokenizer(model_type)\n",
    "dset = TextRetrievalDataset(COCO_PATH, dataset)\n",
    "loader = torch.utils.data.DataLoader(dset, batch_size=512, collate_fn=collate_fn, shuffle=True, num_workers=4, pin_memory=True, drop_last=False)\n",
    "data_iterator = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d518128-4fd4-4551-bbff-8c227dfac137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from open_clip.tokenizer import decode\n",
    "from PIL import Image\n",
    "from torch.distributions.categorical import Categorical\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "ALIGNMENT_THRESHOLD = 0.75\n",
    "\n",
    "raw_images, raw_texts, _ = next(data_iterator)\n",
    "images = torch.stack([preprocess_val(raw_image) for raw_image in raw_images]).cuda()\n",
    "texts = tokenizer(raw_texts).cuda()\n",
    "image_sparos, image_attns = [], []\n",
    "text_sparos, text_attns = [], []\n",
    "with torch.no_grad():\n",
    "    for model in models:\n",
    "        _image_sparos, _image_attns = model.encode_image(images, normalize=True, return_sparo=True, return_attn=True)\n",
    "        _text_sparos, _text_attns = model.encode_text(texts, normalize=True, return_sparo=True, return_attn=True)\n",
    "        _image_attns = _image_attns.squeeze(-2)\n",
    "        _text_attns = _text_attns.squeeze(-2)\n",
    "        image_sparos.append(_image_sparos)\n",
    "        image_attns.append(_image_attns)\n",
    "        text_sparos.append(_text_sparos)\n",
    "        text_attns.append(_text_attns)\n",
    "image_sparos = torch.stack(image_sparos).transpose(0,1)\n",
    "image_attns = torch.stack(image_attns).transpose(0,1)\n",
    "text_sparos = torch.stack(text_sparos).transpose(0,1)\n",
    "text_attns = torch.stack(text_attns).transpose(0,1)\n",
    "\n",
    "all_alignments = []\n",
    "for image_sparos_per_model, text_sparo_per_model in zip(image_sparos, text_sparos):\n",
    "    alignments_per_model = [(F.normalize(image_sparo, dim=-1) * F.normalize(text_sparo, dim=-1)).sum(-1) for image_sparo, text_sparo in zip(image_sparos_per_model, text_sparo_per_model)]\n",
    "    all_alignments.append(alignments_per_model[-1])\n",
    "all_alignments = torch.stack(all_alignments)\n",
    "\n",
    "bad_positions = texts.argmax(dim=-1, keepdim=True).unsqueeze(-2) == text_attns.argmax(dim=-1)\n",
    "bad_positions |= text_attns.argmax(dim=-1) == 0\n",
    "bad_positions |= all_alignments.unsqueeze(-2) < ALIGNMENT_THRESHOLD\n",
    "bad_samples = bad_positions.squeeze(-2).all(dim=-1)\n",
    "raw_images = [raw_images[i] for i, m in enumerate(~bad_samples) if m]\n",
    "raw_texts = [raw_texts[i] for i, m in enumerate(~bad_samples) if m]\n",
    "images = images[~bad_samples]\n",
    "texts = texts[~bad_samples]\n",
    "image_sparos = image_sparos[~bad_samples]\n",
    "image_attns = image_attns[~bad_samples]\n",
    "text_sparos = text_sparos[~bad_samples]\n",
    "text_attns = text_attns[~bad_samples]\n",
    "bad_positions = bad_positions[~bad_samples]\n",
    "all_alignments = all_alignments[~bad_samples]\n",
    "\n",
    "assert not ((text_attns > 0).sum(dim=-1).squeeze(-2).max(dim=-1).values > texts.argmax(dim=-1)+1).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894c34d-a360-404d-bd61-ba058d78018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_slots = torch.where(~bad_positions.squeeze(-2), all_alignments, 0).mean(dim=0).topk(32).indices\n",
    "top_samples = all_alignments[:, top_slots].mean(dim=-1).topk(16).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6973d39-cba7-4ca0-8980-b788d0629920",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARPNESS = 1.0\n",
    "MAGNITUDE = 0.75\n",
    "\n",
    "red = np.concatenate((np.ones((num_edge_patches,num_edge_patches,1)),\n",
    "                      np.zeros((num_edge_patches,num_edge_patches,1)),\n",
    "                      np.zeros((num_edge_patches,num_edge_patches,1))), axis=-1)\n",
    "\n",
    "for ind in top_slots:\n",
    "    print(f\" === SLOT {ind} === \")\n",
    "    top_samples = all_alignments[:, ind].topk(16).indices\n",
    "    for alignments_ind, alignments_vals in enumerate(all_alignments[top_samples]):\n",
    "        if alignments_vals[ind].item() < 0.0:\n",
    "            continue\n",
    "        raw_image, text, image_sparos_per_model, text_sparo_per_model, image_attn_per_model, text_attn_per_model = (\n",
    "            [raw_images[i] for i in top_samples][alignments_ind],\n",
    "            texts[top_samples][alignments_ind],\n",
    "            image_sparos[top_samples][alignments_ind],\n",
    "            text_sparos[top_samples][alignments_ind],\n",
    "            image_attns[top_samples][alignments_ind],\n",
    "            text_attns[top_samples][alignments_ind],\n",
    "        )\n",
    "        decoded_tokens = []\n",
    "        for token in text:\n",
    "            decoded_token = decode(token.unsqueeze(0))\n",
    "            decoded_tokens.append(decoded_token)\n",
    "            if decoded_token == \"<end_of_text>\":\n",
    "                break\n",
    "        image_sparo, text_sparo, image_attn, text_attn = image_sparos_per_model[0], text_sparo_per_model[0], image_attn_per_model[0], text_attn_per_model[0]\n",
    "        print(f\"Slot image-text alignment: {alignments_vals[ind].item():.3f}\")\n",
    "        im_att = image_attn[ind]\n",
    "        im_att = im_att ** SHARPNESS\n",
    "        im_att /= im_att.max() / MAGNITUDE\n",
    "        tx_att = text_attn[ind]\n",
    "        tx_att = tx_att ** SHARPNESS\n",
    "        tx_att /= tx_att.max()\n",
    "        cls_weight = im_att[0]\n",
    "        patch_weights = im_att[1:].view(num_edge_patches,num_edge_patches)\n",
    "        mask = np.concatenate((red, patch_weights.cpu().numpy()[..., None]), axis=-1)\n",
    "        background = raw_image.convert(\"L\").convert(\"RGBA\")\n",
    "        mask = Image.fromarray(np.uint8(mask * 255), mode=\"RGBA\").resize(background.size, resample=Image.Resampling.BICUBIC)  # BICUBIC, NEAREST\n",
    "        background.paste(mask, (0,0), mask)\n",
    "        display(background)\n",
    "        print(f\"CLS weight: {cls_weight:.2f}\\n\")\n",
    "        for decoded_token, weight in zip(decoded_tokens, tx_att):\n",
    "            if not decoded_token.endswith(\" \") and not decoded_token.endswith(\">\"):\n",
    "                decoded_token = decoded_token + \"-\"\n",
    "            if decoded_token == \"<end_of_text>\":\n",
    "                print(f\"     {decoded_token} {weight.item():.2f}\")\n",
    "                break\n",
    "            elif weight.item() >= 1.0:\n",
    "                print(f\"**** {decoded_token} {weight.item():.2f}\")\n",
    "            elif weight.item() >= 0.75:\n",
    "                print(f\"***  {decoded_token} {weight.item():.2f}\")\n",
    "            elif weight.item() >= 0.5:\n",
    "                print(f\"**   {decoded_token} {weight.item():.2f}\")\n",
    "            elif weight.item() >= 0.25:\n",
    "                print(f\"*    {decoded_token} {weight.item():.2f}\")\n",
    "            else:\n",
    "                print(f\"     {decoded_token} {weight.item():.2f}\")\n",
    "\n",
    "    print(\"\\n==========================\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
